{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Test.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ej9DcbIzExkt"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFecxNS10h9F",
        "outputId": "0fdf438f-6c77-4b3e-f77c-5a0779338c49"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uu92TqU03DT",
        "outputId": "26b6d219-7ef7-4e4a-e4db-08258544d473"
      },
      "source": [
        "%cd 'drive/My Drive/SymbolicMath/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/SymbolicMath\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vBfzwKk05aj"
      },
      "source": [
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import sympy as sp\n",
        "import torch\n",
        "\n",
        "from src.utils import AttrDict\n",
        "from src.envs import build_env\n",
        "from src.model import build_modules\n",
        "\n",
        "from src.utils import to_cuda\n",
        "from src.envs.sympy_utils import simplify\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from functools import partial\n",
        "\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcO_qC0Y09PM"
      },
      "source": [
        "params = params = AttrDict({\n",
        "\n",
        "    # environment parameters\n",
        "    'env_name': 'char_sp',\n",
        "    'int_base': 10,\n",
        "    'balanced': False,\n",
        "    'positive': True,\n",
        "    'precision': 10,\n",
        "    'n_variables': 1,\n",
        "    'n_coefficients': 0,\n",
        "    'leaf_probs': '0.75,0,0.25,0',\n",
        "    'max_len': 512,\n",
        "    'max_int': 5,\n",
        "    'max_ops': 15,\n",
        "    'max_ops_G': 15,\n",
        "    'clean_prefix_expr': True,\n",
        "    'rewrite_functions': '',\n",
        "    'tasks': 'prim_fwd',\n",
        "    'operators': 'add:10,sub:3,mul:10,div:5,sqrt:4,pow2:4,pow3:2,pow4:1,pow5:1,ln:4,exp:4,sin:4,cos:4,tan:4,asin:1,acos:1,atan:1,sinh:1,cosh:1,tanh:1,asinh:1,acosh:1,atanh:1',\n",
        "})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjrigfFl4bce"
      },
      "source": [
        "env = build_env(params)         "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtoAgDxJA9Ap"
      },
      "source": [
        "def read_data(path):\n",
        "  with io.open(path, mode='r', encoding='utf-8') as f:\n",
        "    lines = [line.rstrip().split('|') for line in f]\n",
        "    data = [xy.split('\\t') for _, xy in lines]\n",
        "    data = [xy for xy in data if len(xy) == 2]\n",
        "  return data\n",
        "\n",
        "path = 'data/prim_fwd.txt'\n",
        "data = read_data(path)\n",
        "for i in range(len(data)):\n",
        "    data[i] = tuple([sent.split(\" \") for sent in data[i]])\n",
        "# data[0] would be like : \n",
        "# data[0]\n",
        "# [\"sub Y' pow x INT+ 2\", 'mul div INT+ 1 INT+ 3 pow x INT+ 3']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dyubL4RDAwy"
      },
      "source": [
        "def batch_sequences(x, y, env):\n",
        "      \"\"\"\n",
        "      Take as input a list of n sequences (torch.LongTensor vectors) and return\n",
        "      a tensor of size (slen, n) where slen is the length of the longest\n",
        "      sentence, and a vector lengths containing the length of each sentence.\n",
        "      \"\"\"\n",
        "      lengths_x = torch.LongTensor([len(s) + 2 for s in x])\n",
        "      lengths_y = torch.LongTensor([len(s) + 2 for s in y])\n",
        "      max_length = max(lengths_x.max().item(), lengths_y.max().item())\n",
        "      sent_x = torch.LongTensor(max_length , lengths_x.size(0)).fill_(env.pad_index)\n",
        "      sent_y = torch.LongTensor(max_length, lengths_y.size(0)).fill_(env.pad_index)\n",
        "      assert lengths_x.min().item() > 2\n",
        "      assert lengths_y.min().item() > 2\n",
        "\n",
        "      sent_x[0] = env.eos_index\n",
        "      for i, s in enumerate(x):\n",
        "          sent_x[1:lengths_x[i] - 1, i].copy_(s)\n",
        "          sent_x[lengths_x[i] - 1, i] = env.eos_index\n",
        "\n",
        "      sent_y[0] = env.eos_index\n",
        "      for i, s in enumerate(y):\n",
        "          sent_y[1:lengths_y[i] - 1, i].copy_(s)\n",
        "          sent_y[lengths_y[i] - 1, i] = env.eos_index\n",
        "\n",
        "      return sent_x, sent_y, max_length\n",
        "\n",
        "def collate_fn(elements):\n",
        "    \"\"\"\n",
        "    Collate samples into a batch.\n",
        "    \"\"\"\n",
        "    x, y = zip(*elements)\n",
        "    nb_ops = [sum(int(word in env.OPERATORS) for word in seq) for seq in x]\n",
        "    x = [torch.LongTensor([env.word2id[w] for w in seq if w in env.word2id]) for seq in x]\n",
        "    y = [torch.LongTensor([env.word2id[w] for w in seq if w in env.word2id]) for seq in y]\n",
        "    x, y, length = batch_sequences(x, y, env)\n",
        "    return (x, length), (y, length), torch.LongTensor(nb_ops)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjD04WFvEMR7"
      },
      "source": [
        "loader = DataLoader(data, batch_size = 32, shuffle= False, collate_fn=collate_fn)\n",
        "# loader.dataset"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLuoNsezE-6z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8298d59-973c-48d4-95bc-a7167f766230"
      },
      "source": [
        "# Go through one loop\n",
        "counter = 0\n",
        "for (x, x_len), (y, y_len), nb_ops in loader:\n",
        "  print(f\"Iteration {counter}\")\n",
        "  print(\"Batched Input:\")\n",
        "  print(x, x_len)\n",
        "  print(\"Batched Labels:\")\n",
        "  print(y, y_len)\n",
        "  print(\"Batched Lengths:\")\n",
        "  print(nb_ops)\n",
        "  print(\"\")\n",
        "  break"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0\n",
            "Batched Input:\n",
            "tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
            "        [67, 67, 67,  ..., 67, 67, 67],\n",
            "        [79, 79, 79,  ..., 79, 79, 79],\n",
            "        ...,\n",
            "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
            "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
            "        [ 1,  1,  1,  ...,  1,  1,  1]]) 44\n",
            "Batched Labels:\n",
            "tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
            "        [54, 33, 33,  ..., 54, 54, 54],\n",
            "        [47, 54, 54,  ..., 47, 47, 47],\n",
            "        ...,\n",
            "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
            "        [ 1,  1,  1,  ...,  1,  1,  1],\n",
            "        [ 1,  1,  1,  ...,  1,  1,  1]]) 44\n",
            "Batched Lengths:\n",
            "tensor([2, 3, 2, 3, 3, 3, 3, 4, 3, 5, 3, 4, 3, 5, 5, 5, 4, 5, 3, 4, 4, 4, 3, 4,\n",
            "        5, 5, 5, 4, 4, 5, 6, 3])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_rq-1BE8Iz8",
        "outputId": "b5234124-3f9d-4e16-88e0-d5972ed00205"
      },
      "source": [
        "print('batched input shape:', x.shape)\n",
        "print('batched output shape:', y.shape)\n",
        "# each column is showing one training example"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batched input shape: torch.Size([44, 32])\n",
            "batched output shape: torch.Size([44, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM4CtvOACS0Z",
        "outputId": "2ee5f0df-f30e-4d3d-d47b-b390cf9c500a"
      },
      "source": [
        "gpt2 = GPT2Model.from_pretrained('gpt2')\n",
        "in_layer = nn.Embedding(len(env.word2id), 768)\n",
        "out_layer = nn.Linear(768, len(env.word2id)) # or flattening or softmax or non-linear !\n",
        "# [1, 45, 76, 2, 4] = ['sin', 'sub',...]\n",
        "# sin     sub     cos     add\n",
        "# 0.9     0.05   0.05       0  -> sin 0.9 =  1/sum(1+45+...)\n",
        "# 0       0.8     0.2       0 -> sub"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNWEZguM_MRq"
      },
      "source": [
        "for name, param in gpt2.named_parameters():\n",
        "    # freeze all parameters except the layernorm and positional embeddings\n",
        "    if 'ln' in name or 'wpe' in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv6bpBJ4_SJJ"
      },
      "source": [
        "params = list(gpt2.parameters()) + list(in_layer.parameters()) + list(out_layer.parameters())\n",
        "optimizer = torch.optim.Adam(params)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAnX8k-x_bFz"
      },
      "source": [
        "for layer in (gpt2, in_layer):\n",
        "    layer.train()"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "-OTvraO8fv8Z",
        "outputId": "dcfd1807-e443-4f7e-c9dc-f9a48376352e"
      },
      "source": [
        "accuracies = [0]\n",
        "# while sum(accuracies[-50:]) / len(accuracies[-50:]) < .99:\n",
        "for i in range(1):\n",
        "  for (x, x_len), (y, y_len), nb_ops in loader:\n",
        "    \n",
        "    embeddings = in_layer(x.reshape(32, -1))\n",
        "    hidden_state = gpt2(inputs_embeds=embeddings).last_hidden_state[:,-max(y_len):]\n",
        "    logits = out_layer(hidden_state)[0]\n",
        "\n",
        "    loss = loss_fn(logits, y)\n",
        "    accuracies.append((logits.argmax(dim=-1) == y).float().mean().item())\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if len(accuracies) % 500 == 0:\n",
        "        accuracy = sum(accuracies[-50:]) / len(accuracies[-50:])\n",
        "        print(f'Samples: {len(accuracies)}, Accuracy: {accuracy}')\n",
        "\n",
        "print(logits)\n",
        "print(f'Final accuracy: {sum(accuracies[-50:]) / len(accuracies[-50:])}')"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-c90ddee851cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    758\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m                 )\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         )\n\u001b[1;32m    298\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY7BUwrPoxGg",
        "outputId": "ecf5d00a-05d8-479c-e7e6-ea73c905f386"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([44, 32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15IdfOc_oyh4",
        "outputId": "c29db563-a2b1-4b27-b3df-5437fda2efe5"
      },
      "source": [
        "logits.shape"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([44, 91])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cgf7zGJhpp_",
        "outputId": "965d2881-4adb-4df1-c8a4-b56622ff1a6e"
      },
      "source": [
        "hidden_state.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 20, 768])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    }
  ]
}